{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "w6K7xa23Elo4"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rudraashish7/Capstone-Project-Supervised-ML-Regression-Yess-Bank-Stock-Closing-Price-Prediction-/blob/main/Yes_Bank_Stock_Closing_Price_Prediction_capstone_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Team\n",
        "##### **Team Member 1 -**\n",
        "##### **Team Member 2 -**\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes Bank is a well-known bank in the Indian financial domain. Since 2018, it has been in the news because of the fraud case involving Rana Kapoor. Owing to this fact, it was interesting to see how that impacted the stock prices of the company and whether Time series models or any other predictive models can do justice to such situations. This dataset has monthly stock prices of the bank since its inception and includes closing, starting, highest, and lowest stock prices of every month.\n",
        "Our main objective is to predict the stockâ€™s closing price of the month."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write Problem Statement Here.**"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import library\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from datetime import datetime\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.linear_model import Lasso"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "RA8qhoxO7SG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "stock_df = pd.read_csv(\"/content/drive/MyDrive/capstone project-Supervised ML-Regression/Data File /data_YesBank_StockPrices.csv\")"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "# viewing first 5 data set\n",
        "stock_df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stock_df.shape"
      ],
      "metadata": {
        "id": "AGbykYKR9w15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stock_df.info()"
      ],
      "metadata": {
        "id": "XwJl5TNP9YSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for null values.\n",
        "stock_df.isna().sum()"
      ],
      "metadata": {
        "id": "NCz-B6eXUtaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting descriptive statistics of the data.\n",
        "stock_df.describe(include='all')"
      ],
      "metadata": {
        "id": "40qE_220U0WS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let us now preserve the original data before we operate on it.\n",
        "preserved_stock_data = stock_df.copy()"
      ],
      "metadata": {
        "id": "xYFDy4NVVBX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for duplicate instances.\n",
        "stock_df[stock_df.duplicated()==True]"
      ],
      "metadata": {
        "id": "e58dpRJBVF1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# So there is no duplicate data in our dataframe.\n",
        "# checking the datatypes once more.\n",
        "stock_df.dtypes"
      ],
      "metadata": {
        "id": "S51qkXK5VShD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# as we can see, Date column has the object datatype.\n",
        "stock_df['Date']"
      ],
      "metadata": {
        "id": "QAb6NS-TVYDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains various features that provide information about the monthly stock prices of Yes Bank. These features offer valuable insights and details about the stock performance.:-\n",
        "\n",
        "*   `Date` :- The dataset includes the date information in the format of month and year, indicating the specific time period for which the stock prices of Yes Bank are recorded.\n",
        "*   `Open` :- The dataset includes the initial stock price at the start of a specific time period.\n",
        "*   `High` :- The dataset captures the highest price at which the stock traded during the given period, also known as the peak or maximum price.\n",
        "*   `Low` :- The dataset records the lowest price at which the stock traded during the specified period, often referred to as the minimum or bottom price.\n",
        "*   `Close` :- The dataset includes the closing trading price, specifically at the end of each month.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-77Z0kHsTq-O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# After conducting an analysis of the dataset, we can draw several conclusions based on the findings.\n",
        "\n",
        "\n",
        "\n",
        "*   The dataset is structured with five distinct features, each providing valuable information about the data points. In total, there are 185 data points available for analysis, allowing for a comprehensive examination of the dataset.\n",
        "*   The dataset is free from any null or missing data points, indicating that all the required information is available for analysis. This absence of null data ensures the reliability and completeness of the dataset, enabling accurate and comprehensive exploration of the provided information.\n",
        "*   Among the columns in the dataset, only the \"Date\" feature is represented as an object/string datatype. This means that the dates are stored as text rather than numerical values. The remaining features are likely represented as numerical data types, such as integers or floats. The inclusion of the \"Date\" feature allows for temporal analysis and tracking of the stock prices over time.\n",
        "\n"
      ],
      "metadata": {
        "id": "KAvg4jBCVsUy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Wrangling"
      ],
      "metadata": {
        "id": "Zz_xYEZzYSHl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After thoroughly examining the dataset, we have determined that it contains no null or duplicate values, indicating its overall data integrity. However, there is a need to address the \"Date\" feature, which is currently stored as an object datatype. In order to utilize the dataset effectively across various models and actions, it is necessary to convert the \"Date\" feature to a proper datetime data type. Additionally, setting the \"Date\" feature as the index will enable easier manipulation and analysis of the dataset, ensuring its compatibility with different modeling techniques and actions."
      ],
      "metadata": {
        "id": "ICTcU1-VYbWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we need to modify this before passing it to a model.\n",
        "# lets convert Date column to a proper datetime datatype.\n",
        "stock_df['Date'] = pd.to_datetime(stock_df['Date'].apply(lambda x: datetime.strptime(x, '%b-%y')))     # this converts date to a yyyy-mm-dd format."
      ],
      "metadata": {
        "id": "CFsMeGAEVsBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rechecking the dataset\n",
        "stock_df.head()"
      ],
      "metadata": {
        "id": "kTblTzjIVre_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Given our objective of tracking variations in stock prices across different dates, it is logical to set the \"Date\" column as the index of the dataset. By doing so, we can easily access and analyze the stock price data based on specific dates. This indexing approach allows us to efficiently navigate and retrieve information, facilitating a comprehensive understanding of the stock price fluctuations over time."
      ],
      "metadata": {
        "id": "SGO7baCOZ_eD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stock_df.set_index('Date', inplace=True)           # setting Date column as index."
      ],
      "metadata": {
        "id": "ANnMZXNlZ2Jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the data.\n",
        "stock_df.head()"
      ],
      "metadata": {
        "id": "YqKrN3q4aC8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upon examining the dataframe provided above, it is evident that all the columns exclusively consist of numerical data. There is an absence of any categorical data in the dataset. This means that the information available for analysis primarily comprises quantitative values, allowing for numerical calculations, statistical analyses, and modeling techniques to be applied directly to the dataset. The lack of categorical data simplifies the data processing and ensures a streamlined approach when performing quantitative analyses."
      ],
      "metadata": {
        "id": "ZyhkhNa8aS6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking all features for presence of outliers.\n",
        "for col in stock_df.columns:\n",
        "  fig, ax = plt.subplots(figsize=(7, 5))\n",
        "  sns.boxplot(x=stock_df[col], ax=ax, color='lightblue')\n",
        "  ax.set_xlabel(col, fontsize=13)\n",
        "  ax.set_title(f\"Boxplot of {col} feature\", fontsize=15)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "EQyyKzeLaI09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upon observation, it is evident that our dataset contains outliers. These outliers are data points that significantly deviate from the majority of the data. Before proceeding with modeling or further analysis, it is essential to address these outliers. Dealing with outliers involves identifying their impact on the data and deciding on an appropriate course of action, such as removing or transforming them. By addressing the outliers, we can ensure the robustness and reliability of our models and analyses."
      ],
      "metadata": {
        "id": "KsTaQJWHbTnY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now let's perform Exploratory Data Analysis(EDA)"
      ],
      "metadata": {
        "id": "c2bhBZDXbqFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separating the dependent and independent variables.\n",
        "# Get the list of independent variables and the dependent variable from the stock_df dataframe.\n",
        "independent_variables = stock_df.columns[:-1].tolist() # Get all columns except the last one\n",
        "dependent_variable = ['Close'] # The target variable for prediction is 'Close'\n",
        "\n",
        "# Print the list of independent variables and the dependent variable.\n",
        "print(\"Independent Variables:\", independent_variables)\n",
        "print(\"Dependent Variable:\", dependent_variable)"
      ],
      "metadata": {
        "id": "6Y6BDwiuapPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the dependent variable (Closing Price)\n",
        "\n",
        "plt.figure(figsize=(12, 7)) # Setting the figure size\n",
        "\n",
        "stock_df['Close'].plot(color='b') # Ploting the 'Close' column from stock_df dataframe\n",
        "\n",
        "plt.grid(which='both', linestyle='-', linewidth='0.2', color='red') # Adding gridlines to the plot\n",
        "plt.xlabel('Date') # Setting the x-axis label\n",
        "plt.ylabel('Closing Price') # Setting the y-axis label\n",
        "plt.title('Closing Price Over Date') # Setting the title of the plot\n",
        "\n",
        "plt.show() # Displaying the plot"
      ],
      "metadata": {
        "id": "4i8uzF_vb9W-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the distributions of all features.\n",
        "for col in stock_df.columns:\n",
        "  plt.figure(figsize=(10, 6)) # Set the figure size\n",
        "  # Plot the distribution using a histogram and kernel density estimation\n",
        "  sns.distplot(stock_df[col], color='r', kde = True)\n",
        "\n",
        "  plt.xlabel(col, fontsize=13)  # Set the x-axis label\n",
        "  plt.ylabel('Count')  # Set the y-axis label\n",
        "\n",
        "  # Plotting the mean and median as vertical lines\n",
        "  plt.axvline(stock_df[col].mean(), color='green', linewidth=2)  # Mean line in green\n",
        "  plt.axvline(stock_df[col].median(), color='black', linestyle='dashed', linewidth=1.5)  # Median line in red with dashed style\n",
        "  plt.title(f\"Distribution of {col}\", fontsize=15) # Set the title of the graph\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "1RyK2bVAcZ0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upon analyzing the distributions, it is evident that they exhibit positive skewness, indicated by the longer tail on the right side. The noticeable difference between the mean and median further emphasizes this skewness. In order to achieve optimal results with our models, it is crucial to transform these distributions to approximate a normal distribution.\n",
        "\n",
        "To accomplish this, we can apply various transformation techniques such as logarithmic, square root, or Box-Cox transformations. These transformations aim to normalize the data by reducing the impact of skewness and aligning the distribution closer to a normal distribution. By doing so, we can enhance the performance of our models, as many statistical techniques assume normality for accurate and reliable predictions."
      ],
      "metadata": {
        "id": "eXeqo3jwgBap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply log transformation on the features and plot the transformed distributions\n",
        "for col in stock_df.columns:\n",
        "  plt.figure(figsize=(10, 6)) # Set the figure size\n",
        "  # Plot the distribution of the log-transformed feature using a histogram and kernel density estimation\n",
        "  sns.histplot(np.log10(stock_df[col]), color='b', kde=True)\n",
        "\n",
        "  plt.xlabel(col, fontsize=13)  # Set the x-axis label\n",
        "  plt.ylabel('Count')  # Set the y-axis label\n",
        "\n",
        "  # Plotting the mean and median of the log-transformed feature as vertical lines\n",
        "  plt.axvline(np.log10(stock_df[col]).mean(), color='green', linewidth=2)  # Mean line in green\n",
        "  plt.axvline(np.log10(stock_df[col]).median(), color='red', linestyle='dashed', linewidth=1.5)  # Median line in red with dashed style\n",
        "  plt.title(f\"Distribution of {col}\", fontsize=15) # Set the title of the graph\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "SDjyi0IefMLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After applying the logarithmic transformation to the features, we can observe that the distributions have become much closer to a normal distribution. The shapes of the distributions have been improved, and the skewness has been reduced significantly.\n",
        "\n",
        "Furthermore, the mean and median values of the transformed features are now much closer to each other. This suggests that the data points are more symmetrically distributed around the central tendency. The similarity between the mean and median values indicates a better alignment of the distribution towards normality.\n",
        "\n",
        "These transformed distributions provide a more suitable foundation for our models as they exhibit characteristics that are closer to the assumptions made by many statistical techniques. This normalization process enhances the reliability and accuracy of our models, leading to optimal results in data analysis and predictions."
      ],
      "metadata": {
        "id": "o7bKvPmWhZKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for outliers in the transformed variable data\n",
        "for col in stock_df.columns:\n",
        "  plt.figure(figsize=(7, 7)) # Set the figure size\n",
        "  sns.boxplot(x=np.log10(stock_df[col]), color='lightblue') # Boxplot of the log-transformed variable with light blue color\n",
        "  plt.xlabel(col, fontsize=13) # Set the x-axis label\n",
        "  plt.title(f\"Boxplot of log-transformed {col}\", fontsize=15) # Set the title of the graph\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "0aYLHJMxgwPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After applying the log transformation, we can observe that the presence of outliers has diminished significantly. The log transformation helps reduce the impact of outliers on the data by compressing the values towards the center of the distribution.\n",
        "\n",
        "However, considering the small size of our dataset, completely dropping the outliers is not advisable. With limited data points, removing outliers may result in a loss of valuable information and potentially bias our analysis. Therefore, it is prudent to retain the outliers in our dataset and proceed with the analysis as they can still provide important insights and contribute to the overall understanding of the data."
      ],
      "metadata": {
        "id": "PN114IvqikvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the independent variables against dependent variable close and also checking the correlation between them.\n",
        "for col in independent_variables:\n",
        "  fig, ax = plt.subplots(figsize=(12, 6))\n",
        "  feature = stock_df[col]\n",
        "  label = stock_df['Close']\n",
        "  correlation = feature.corr(label) # Calculating the correlation between the independent variable and 'Close'\n",
        "  ax.scatter(x=feature, y=label,color='orange') # Plotting the independent variable against 'Close'\n",
        "  # Setting the x-label, y-label, and title\n",
        "  ax.set_xlabel(col)\n",
        "  ax.set_ylabel('Close')\n",
        "  ax.set_title('Close vs ' + col + ' - Correlation: ' + str(round(correlation, 4)))\n",
        "\n",
        "  # Fitting a linear regression line\n",
        "  z = np.polyfit(stock_df[col], stock_df['Close'], 1)\n",
        "  y_hat = np.poly1d(z)(stock_df[col])\n",
        "  ax.plot(stock_df[col], y_hat, \"r--\",color='blue', lw=1)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YBXsMIFIh5Lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upon analyzing the plots, it is evident that all of the independent variables exhibit a strong correlation with the dependent variable ('Close'). This indicates a significant relationship between the independent and dependent variables in our dataset.\n",
        "\n",
        "Furthermore, the relationship between the dependent variable and independent variables appears to be linear in nature. This suggests that a linear regression model might be suitable for capturing and predicting the relationship between these variables. The linear regression line fitted in each plot reinforces the linear trend observed between the variables.\n",
        "\n",
        "The high correlations and linear relationships observed in the plots imply that the independent variables hold valuable predictive power for determining the dependent variable ('Close'). These findings contribute to our understanding of the dataset and provide insights for selecting appropriate modeling techniques."
      ],
      "metadata": {
        "id": "iGuiIqmrkt1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the correlation matrix\n",
        "corr = stock_df.corr()\n",
        "\n",
        "plt.figure(figsize=(16, 7)) # Set the figure size\n",
        "\n",
        "# Create a heatmap of the correlation matrix\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
        "\n",
        "plt.title('Correlation Heatmap') # Set the title of the heatmap\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gH_hPioRk1S9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upon examining the heatmap, it becomes evident that there exists a significant degree of correlation between each pair of features in our dataset. While a high correlation between the dependent variable and independent variables is desirable for effective prediction, it is equally important to avoid high correlations among the independent variables themselves.\n",
        "\n",
        "The presence of high correlation among independent variables, known as multicollinearity, poses a challenge for our models. Multicollinearity can lead to issues such as unstable coefficient estimates, difficulty in determining the individual impact of each variable, and reduced model interpretability.\n",
        "\n",
        "In our case, the observed high correlation between independent variables indicates a potential multicollinearity issue. This calls for appropriate measures to address multicollinearity, such as feature selection, dimensionality reduction, or transforming variables to reduce correlation.\n",
        "\n",
        "By addressing multicollinearity, we can enhance the effectiveness and interpretability of our models and improve the accuracy of our predictions."
      ],
      "metadata": {
        "id": "BEsQOpeKllNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's visualise the relationship between each pair of variables using pair plots.\n",
        "sns.pairplot(stock_df)"
      ],
      "metadata": {
        "id": "lavQCkzPlEiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "wKY9urFQlvln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dealing with multicollinearity using VIF analysis.\n",
        "# Calculating VIF(Variation Inflation Factor) to see the correlation between independent variables\n",
        "\n",
        "# Calculate the VIF (Variance Inflation Factor) for each independent variable\n",
        "X = stock_df[[col for col in stock_df.describe().columns if col not in ['Date', 'Close']]]\n",
        "vif = pd.DataFrame()\n",
        "vif[\"Feature\"] = X.columns\n",
        "vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "print(vif)"
      ],
      "metadata": {
        "id": "7PIVMo8KlpSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upon analyzing the VIF (Variance Inflation Factor) values, it is apparent that the values are quite high. However, it is important to note that our dataset is relatively small and contains only three independent features. In such cases, multicollinearity can become unavoidable due to the limited number of variables.\n",
        "\n",
        "Attempting feature engineering or applying transformations to address multicollinearity in this small dataset could result in a loss of valuable information. It is crucial to strike a balance between addressing multicollinearity and preserving the integrity of the data.\n",
        "\n",
        "While it is desirable to mitigate multicollinearity whenever possible, the small size of the dataset limits our options in terms of feature selection or dimensionality reduction techniques. In such situations, it becomes necessary to carefully interpret the model results, considering the potential impact of multicollinearity on coefficient estimates and model performance.\n",
        "\n",
        "Given the constraints of the dataset, it is important to acknowledge the presence of multicollinearity and proceed with model development while being cautious of its potential implications."
      ],
      "metadata": {
        "id": "cUofoNwamlZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating arrays of our input variable and label to feed the data to the model.\n",
        "# Create the data of independent variables\n",
        "x = np.log10(stock_df[independent_variables]).values            # applying log transform on our independent variables.\n",
        "\n",
        "# Create the dependent variable data\n",
        "y = np.log10(stock_df[dependent_variable]).values               # applying log transform on our dependent variable."
      ],
      "metadata": {
        "id": "zKWdX-oOmk-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting the data into a train and a test set. we do this using train test split.\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)       # we keep 20% of the data in test set."
      ],
      "metadata": {
        "id": "WbNIoKQxm1GG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our case, scaling the data is crucial to prevent features with large values from dominating the modeling process. This can be achieved through the process of normalization or standardization, which helps to bring the features to a comparable scale.\n",
        "\n",
        "Normalization involves rescaling the features to a range between 0 and 1, preserving the relative relationships between the data points. This is useful when the distribution of the data is not necessarily Gaussian.\n",
        "\n",
        "On the other hand, standardization transforms the data to have a mean of 0 and a standard deviation of 1. It ensures that the features have zero mean and unit variance, making them more directly comparable and suitable for techniques that assume Gaussian distribution.\n",
        "\n",
        "By scaling the data, we ensure that each feature is given equal importance and prevents any undue influence from features with larger magnitudes. This facilitates more effective modeling and enables reliable comparisons between the variables.\n",
        "\n",
        "It is important to consider the specific characteristics of the data and the requirements of the modeling techniques in order to choose the appropriate scaling method, either normalization or standardization, for our dataset."
      ],
      "metadata": {
        "id": "0O6sGLT_nKWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling the data.\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)"
      ],
      "metadata": {
        "id": "Fn9vjUw8nLGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the values.\n",
        "x_train[0:5]"
      ],
      "metadata": {
        "id": "c-V8ZO5rnU5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Appling Linear regression"
      ],
      "metadata": {
        "id": "3J4sMRwWnb3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the model.\n",
        "model_lr = LinearRegression()\n",
        "\n",
        "# Fitting the model on our train data.\n",
        "model_lr.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "4wINneQjnW7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting on our test data.\n",
        "y_pred_linear = model_lr.predict(x_test)"
      ],
      "metadata": {
        "id": "6PD3jzZunoKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the model parameters. printing the intercept.\n",
        "model_lr.intercept_"
      ],
      "metadata": {
        "id": "RNBrX3hdnsjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# printing the model coefficients.\n",
        "model_lr.coef_"
      ],
      "metadata": {
        "id": "N8Tjnr2bnvU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the performance metrics.\n",
        "MAE_linear = round(mean_absolute_error(10**(y_test),(10**y_pred_linear)),4)\n",
        "print(f\"Mean Absolute Error : {MAE_linear}\")\n",
        "\n",
        "MSE_linear = round(mean_squared_error((10**y_test),10**(y_pred_linear)),4)\n",
        "print(f\"Mean squared Error : {MSE_linear}\")\n",
        "\n",
        "RMSE_linear = round(np.sqrt(MSE_linear),4)\n",
        "print(f\"Root Mean squared Error : {RMSE_linear}\")\n",
        "\n",
        "R2_linear = round(r2_score(10**(y_test), 10**(y_pred_linear)),4)\n",
        "print(f\"R2 score : {R2_linear}\")\n",
        "\n",
        "Adjusted_R2_linear = round(1-(1-r2_score(10**y_test,10**y_pred_linear))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)),3)\n",
        "print(f\"Adjusted R2 score : {Adjusted_R2_linear}\")\n"
      ],
      "metadata": {
        "id": "6MeowS1AnxDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the actual and predicted test data.\n",
        "plt.figure(figsize=(9,5))\n",
        "plt.plot(10**y_pred_linear)\n",
        "plt.plot(np.array(10**y_test))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.xlabel('Test Data')\n",
        "plt.ylabel(\"Price\")\n",
        "plt.title(\"Actual vs Predicted Closing price Linear regression\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eV6xmqmany59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to store our performance data for this model so that we can compare them with other models. Let's store them in a dict for now."
      ],
      "metadata": {
        "id": "IARTg4Nmn38q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "linear_regessor_list = {'Mean Absolute Error' : MAE_linear,'Mean squared Error' : MSE_linear,\n",
        "                   'Root Mean squared Error' : RMSE_linear,'R2 score' : R2_linear,'Adjusted R2 score' : Adjusted_R2_linear }"
      ],
      "metadata": {
        "id": "rgEP3uKAn1Hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting above dict into a dataframe\n",
        "metric_df = pd.DataFrame.from_dict(linear_regessor_list, orient='index').reset_index()"
      ],
      "metadata": {
        "id": "YvQXeFCHn7JT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# renaming the columns.\n",
        "metric_df = metric_df.rename(columns={'index':'Metric',0:'Linear Regression'})\n",
        "metric_df"
      ],
      "metadata": {
        "id": "xQ7Jqsj2n87W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now use this df to store all metrics of all other models so we can easily compare them."
      ],
      "metadata": {
        "id": "-Ln6TjTAoB9K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Lasso Regression with cross validated regularization."
      ],
      "metadata": {
        "id": "XtRv6k4UoCyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the model with some base values.\n",
        "lasso  = Lasso(alpha=0.0001 , max_iter= 3000)\n",
        "# Fitting the model on our training data.\n",
        "lasso.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "3e9CO6pon-zG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the intercept and coefficients.\n",
        "lasso.intercept_"
      ],
      "metadata": {
        "id": "FBYV2ZJeoQGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lasso.coef_"
      ],
      "metadata": {
        "id": "rRGzHIG8oSrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross validation. optimizing our model by finding the best value of our hyperparameter.\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "lasso_param_grid = {'alpha': [1e-15,1e-13,1e-10,1e-8,1e-5,1e-4,1e-3,0.005,0.006,0.007,0.01,0.015,0.02,1e-1,1,5,10,20,30,40,45,50]}  # list of parameters.\n",
        "\n",
        "lasso_regressor = GridSearchCV(lasso, lasso_param_grid, scoring='neg_mean_squared_error', cv=5)\n",
        "lasso_regressor.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "h688ciQjoUGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the best parameter\n",
        "lasso_regressor.best_params_          # after several iterations and trials, we get this value as best parameter value."
      ],
      "metadata": {
        "id": "uqOEgHVaoWEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the best score\n",
        "lasso_regressor.best_score_"
      ],
      "metadata": {
        "id": "p9IuaQmLoaO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting on the test dataset.\n",
        "y_pred_lasso = lasso_regressor.predict(x_test)\n",
        "print(y_pred_lasso)"
      ],
      "metadata": {
        "id": "CHETA2D6ob-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the performance using evaluation metrics.\n",
        "MAE_lasso = round(mean_absolute_error(10**(y_test),10**(y_pred_lasso)),4)\n",
        "print(f\"Mean Absolute Error : {MAE_lasso}\")\n",
        "\n",
        "MSE_lasso  = round(mean_squared_error(10**(y_test),10**(y_pred_lasso)),4)\n",
        "print(\"Mean squared Error :\" , MSE_lasso)\n",
        "\n",
        "RMSE_lasso = round(np.sqrt(MSE_lasso),4)\n",
        "print(\"Root Mean squared Error :\" ,RMSE_lasso)\n",
        "\n",
        "R2_lasso = round(r2_score(10**(y_test), 10**(y_pred_lasso)),4)\n",
        "print(\"R2 score :\" ,R2_lasso)\n",
        "\n",
        "Adjusted_R2_lasso = round(1-(1-r2_score(10**y_test, 10**y_pred_lasso))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)),4)\n",
        "print(\"Adjusted R2 score: \", Adjusted_R2_lasso)"
      ],
      "metadata": {
        "id": "UonOset3odzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now saving these metrics to our metrics dataframe. First we save them in a list and then we pass them to the df.\n",
        "metric_df['Lasso'] = [MAE_lasso, MSE_lasso, RMSE_lasso, R2_lasso, Adjusted_R2_lasso]"
      ],
      "metadata": {
        "id": "eL-biP0Bognu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting the predicted values vs actual.\n",
        "plt.figure(figsize=(9,5))\n",
        "plt.plot(10**y_pred_lasso)\n",
        "plt.plot(np.array(10**y_test))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.ylabel(\"Price\")\n",
        "plt.title(\"Actual vs Predicted Closing price Lasso regression\")"
      ],
      "metadata": {
        "id": "AIZ2YF18ojS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Ridge Regression with cross validated regularization."
      ],
      "metadata": {
        "id": "eihRfvejonBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing ridge regressor model.\n",
        "from sklearn.linear_model import Ridge\n",
        "ridge = Ridge()         # iitializing the model\n",
        "\n",
        "# initiating the parameter grid for alpha (regularization strength).\n",
        "ridge_param_grid = {'alpha': [1e-15,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,0.3,0.7,1,1.2,1.33,1.365,1.37,1.375,1.4,1.5,1.6,1.8,2.5,5,10,20,30,40,45,50,55,60,100]}\n",
        "\n",
        "# cross validation.\n",
        "ridge_regressor = GridSearchCV(ridge, ridge_param_grid, scoring='neg_mean_squared_error', cv=3)\n",
        "ridge_regressor.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "OPcZoItrolX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finding the best parameter value (for alpha)\n",
        "ridge_regressor.best_params_"
      ],
      "metadata": {
        "id": "jXUL9qFNorsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the best score for optimal value of alpha.\n",
        "ridge_regressor.best_score_"
      ],
      "metadata": {
        "id": "vzH1nQpzot-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predicting on the test dataset now.\n",
        "y_pred_ridge = ridge_regressor.predict(x_test)"
      ],
      "metadata": {
        "id": "m1-W6SLKovkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluating performance.\n",
        "MAE_ridge = round(mean_absolute_error(10**(y_test),10**(y_pred_ridge)),4)\n",
        "print(f\"Mean Absolute Error : {MAE_ridge}\")\n",
        "\n",
        "MSE_ridge  = round(mean_squared_error(10**(y_test),10**(y_pred_ridge)),4)\n",
        "print(\"Mean squared Error :\" , MSE_ridge)\n",
        "\n",
        "RMSE_ridge = round(np.sqrt(MSE_ridge),4)\n",
        "print(\"Root Mean squared Error :\" ,RMSE_ridge)\n",
        "\n",
        "R2_ridge = round(r2_score(10**(y_test), 10**(y_pred_ridge)),4)\n",
        "print(\"R2 score :\" ,R2_ridge)\n",
        "\n",
        "Adjusted_R2_ridge = round(1-(1-r2_score(10**y_test, 10**y_pred_ridge))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)),4)\n",
        "print(\"Adjusted R2 score: \", Adjusted_R2_ridge)"
      ],
      "metadata": {
        "id": "gA548hUyoxpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# storing these values in a list and appending to our metric df.\n",
        "ridge_regressor_list = [MAE_ridge,MSE_ridge,RMSE_ridge,R2_ridge,Adjusted_R2_ridge]\n",
        "metric_df['Ridge'] = ridge_regressor_list"
      ],
      "metadata": {
        "id": "aG0AzFProzJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting predicted and actual target variable values.\n",
        "plt.figure(figsize=(9,5))\n",
        "plt.plot(10**y_pred_ridge)\n",
        "plt.plot(np.array(10**y_test))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.ylabel(\"Price\")\n",
        "plt.title(\"Actual vs Predicted Closing price Ridge regression\")"
      ],
      "metadata": {
        "id": "SEdxHwQxo1f4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Elastic-Net Regression with cross validation."
      ],
      "metadata": {
        "id": "YgMpD0Uao6CS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing and initializing Elastic-Net Regression.\n",
        "from sklearn.linear_model import ElasticNet\n",
        "elasticnet_model = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
        "\n",
        "# initializing parameter grid.\n",
        "elastic_net_param_grid = {'alpha': [1e-15,1e-13,1e-10,1e-8,1e-5,1e-4,1e-3,0.001,0.01,0.02,0.03,0.04,1,5,10,20,40,50,60,100],\n",
        "                          'l1_ratio':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]}\n",
        "\n",
        "# cross-validation.\n",
        "elasticnet_regressor = GridSearchCV(elasticnet_model, elastic_net_param_grid, scoring='neg_mean_squared_error',cv=5)\n",
        "elasticnet_regressor.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "OyMyBmj-o3Vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finding the best parameter\n",
        "elasticnet_regressor.best_params_"
      ],
      "metadata": {
        "id": "idID31YTo9f5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finding the best score for the optimal parameter.\n",
        "elasticnet_regressor.best_score_"
      ],
      "metadata": {
        "id": "Zqg0XeCWpA19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# making the predictions.\n",
        "y_pred_elastic_net = elasticnet_regressor.predict(x_test)"
      ],
      "metadata": {
        "id": "12GZhxyFpEMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAE_elastic_net = round(mean_absolute_error(10**(y_test),10**(y_pred_elastic_net)),4)\n",
        "print(f\"Mean Absolute Error : {MAE_elastic_net}\")\n",
        "\n",
        "MSE_elastic_net  = round(mean_squared_error(10**(y_test),10**(y_pred_elastic_net)),4)\n",
        "print(\"Mean squared Error :\" , MSE_elastic_net)\n",
        "\n",
        "RMSE_elastic_net = round(np.sqrt(MSE_elastic_net),4)\n",
        "print(\"Root Mean squared Error :\" ,RMSE_elastic_net)\n",
        "\n",
        "R2_elastic_net = round(r2_score(10**(y_test), (10**y_pred_elastic_net)),4)\n",
        "print(\"R2 score :\" ,R2_elastic_net)\n",
        "\n",
        "Adjusted_R2_elastic_net = round(1-(1-r2_score(10**y_test, 10**y_pred_elastic_net))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)),4)\n",
        "print(\"Adjusted R2 score: \", Adjusted_R2_elastic_net)"
      ],
      "metadata": {
        "id": "nKceZ4dwpGmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# storing these metrics in our dataframe.\n",
        "elastic_net_metric_list = [MAE_elastic_net,MSE_elastic_net,RMSE_elastic_net,R2_elastic_net,Adjusted_R2_elastic_net]\n",
        "metric_df['Elastic Net'] = elastic_net_metric_list"
      ],
      "metadata": {
        "id": "YUNV9kZIpI0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let us plot the actual and predicted target variables values.\n",
        "plt.figure(figsize=(9,5))\n",
        "plt.plot(10**y_pred_elastic_net)\n",
        "plt.plot(np.array(10**y_test))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.ylabel(\"Price\")\n",
        "plt.title(\"Actual vs Predicted Closing price Elastic Net regression\")"
      ],
      "metadata": {
        "id": "AZqfxCqepSJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# comparing the performance of all models that we have implemented.\n",
        "metric_df"
      ],
      "metadata": {
        "id": "0_lOpYX0pUUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above data, we can clearly see that the best performing model is Elastic Net as it scores the best in every single metric."
      ],
      "metadata": {
        "id": "QxGKH8_ipZkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the predicted values of all the models against the true values.\n",
        "plt.figure(figsize=(16,8))\n",
        "plt.plot(10**y_test, linewidth=2)\n",
        "plt.plot(10**y_pred_linear)\n",
        "plt.plot(10**y_pred_lasso)\n",
        "plt.plot(10**y_pred_ridge)\n",
        "plt.plot(10**y_pred_elastic_net)\n",
        "plt.legend(['linear','lasso','ridge','elastic_net'])\n",
        "plt.title('Actual vs Predicted Closing Price values by various Algorithms', weight = 'bold',fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EUlhzWvbpXOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see from above graph, all of our models are performing really well and are able to closely approximate the actual values."
      ],
      "metadata": {
        "id": "uvWLY4WLph84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets check for Heterodasticity. Homoscedasticity is an assumption in linear regression algorithm.\n",
        "# Homoscedasticity means that the model should perform well on all the datapoints.\n",
        "\n",
        "# Plotting the residuals(errors) against actual test data.\n",
        "residuals = 10**y_test - 10**y_pred_elastic_net.reshape(37,1)\n",
        "plt.scatter(10**y_test,residuals,c='red')\n",
        "plt.title('Actual Test data vs Residuals (Elastic Net)')\n"
      ],
      "metadata": {
        "id": "Sx_v52aZpdxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above graph, we have plotted the residuals against test set value for our actual test set values for our best performing model (Elastic Net Regressor).\n",
        "\n",
        "As we can see, there is no discernable pattern here in the plot. The errors are similar for all datapoints and the model is performing equally well on all datapoints. So we can say that the assumption of Homoscedasticity is valid in this case."
      ],
      "metadata": {
        "id": "tEx_ModTpnCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the actual and elastic net predicted target variables values in a dataframe.\n",
        "actual_pred_df = pd.DataFrame(10**y_test,10**y_pred_elastic_net).reset_index().rename(columns = {'index':'Actual values',0:'Elastic Net Predicted values'})\n",
        "actual_pred_df.head()"
      ],
      "metadata": {
        "id": "2xA9ocwEpkjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusions\n",
        "\n",
        "\n",
        "*   Using data visualization on our target variable, we can clearly see the impact of 2018 fraud case involving Rana Kapoor as the stock prices decline dramatically during that period.\n",
        "*   After loading the dataset, we found that there are no null values in our dataset nor any duplicate data.\n",
        "*   There are some outliers in our features however this being a very small dataset, dropping those instances will lead to loss of information.\n",
        "*   We found that the distribution of all our variables is positively skewed. so we performed log transformation on them.\n",
        "*   There is a high correlation between the dependent and independent variables. This is a signal that our dependent variable is highly dependent on our features and can be predicted accurately from them.\n",
        "*   We found that there is a rather high correlation between our independent variables. This multicollinearity is however unavoidable here as the dataset is very small.\n",
        "*   We implemented several models on our dataset in order to be able to predict the closing price and found that all our models are performing remarkably well and Elastic Net regressor is the best performing model with Adjusted R2 score value of 0.9932 and scores well on all evaluation metrics.\n",
        "*   All of the implemented models performed quite well on our data giving us the Adjusted R-square of over 99%.\n",
        "*   We checked for presence of Heterodasceticity in our dataset by plotting the residuals against the Elastic Net model predicted value and found that there is no Heterodasceticity present. Our model is performing well on all data-points.\n",
        "*   With our model making predictions with such high accuracy, we can confidently deploy this model for further predictive tasks using future data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SZviySJepuwQ"
      }
    }
  ]
}